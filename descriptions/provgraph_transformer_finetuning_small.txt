A provenance file created using the yProv4ML library (version 0), following the W3C prov schema (http://www.w3.org/2000/10/XMLSchema#), and creating a user namespace at www.example.org. The file contains a machine learning process with run_id: 0, started by user lelepado, with python 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0], and named transformer_finetuning. 
The entire experiment was run in a None environment, The experiment was scheduled at 2025-03-13 16:38:00 and it finished at 2025-03-13 16:40:41, for a total of 160.75 seconds. 
The model was trained for 12 epochs,each epoch consisting in the passthrough of 2000 samples, with batch size 32. The model was saved 1 times, in the checkpoints prov/transformer_finetuning_0/artifacts/vit_cifar100_final/vit_cifar100_final_0.pth. with the final version being vit_cifar100_final. The final model has a total of 85875556 parameters, and the footprint on the memory is of 343.5 Mb. The datasets used for the training of the model are: train_dataset with 2000 samples, 32 batch size and 63 steps; val_dataset with 64 samples, 32 batch size and 2 steps. 
The user saved a set of parameters from the process, including: execution_start_time, optimizer, loss_fn, model_name, total_params, memory_of_model, total_memory_load_of_model, execution_end_timeA set of metrics have been collected from the process, in particular in the contexts VALIDATION, TRAINING, these metrics are gpu_power_usage_Context.TRAINING, ram_power_Context.TRAINING, gpu_temperature_Context.TRAINING, cpu_energy_Context.TRAINING, gpu_energy_Context.TRAINING, cpu_usage_Context.TRAINING, energy_consumed_Context.TRAINING, gpu_power_Context.TRAINING, memory_usage_Context.TRAINING, disk_usage_Context.TRAINING, ram_energy_Context.TRAINING, emissions_Context.TRAINING, Loss_Context.TRAINING, emissions_rate_Context.TRAINING, gpu_memory_usage_Context.TRAINING, Loss_Context.VALIDATION, cpu_power_Context.TRAINING, gpu_usage_Context.TRAINING. 